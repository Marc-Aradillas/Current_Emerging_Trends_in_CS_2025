{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3-2  Identifying CIFAR-10 Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marc Aradillas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR10 is a set of 60K images 32x32 pixels on 3 channels\n",
    "IMG_CHANNELS = 3\n",
    "IMG_ROWS = 32\n",
    "IMG_COLS = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "BATCH_SIZE = 128\n",
    "NB_EPOCH = 20\n",
    "NB_CLASSES = 10\n",
    "VERBOSE = 1\n",
    "VALIDATION_SPLIT = 0.2\n",
    "OPTIM = RMSprop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nError SSL certificate so modified code for workaround on example.\\n\\nException: URL fetch failure on https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz : \\nNone -- [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local \\nissuer certificate (_ssl.c:1076)\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Load CIFAR-10 dataset\n",
    "# (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "# print(\"X_train shape:\", X_train.shape)\n",
    "# print(X_train.shape[0], 'train samples')\n",
    "# print(X_test.shape[0], 'test samples')\n",
    "\n",
    "'''\n",
    "Error SSL certificate so modified code for workaround on example. extracted tar.gz file and pasted into the \"\\datasets\" directory\n",
    "\n",
    "Exception: URL fetch failure on https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz : \n",
    "None -- [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local \n",
    "issuer certificate (_ssl.c:1076)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the local CIFAR-10 dataset.\n",
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 3s 0us/step\n",
      "X_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ssl\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "# Bypass SSL certificate verification\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Check if the CIFAR-10 dataset exists locally\n",
    "cifar10_dataset_path = r\"C:\\Users\\marcaradillas_snhu\\.keras\\datasets\\cifar-10-batches-py\"\n",
    "\n",
    "if os.path.exists(cifar10_dataset_path):\n",
    "    print(\"Using the local CIFAR-10 dataset.\")\n",
    "    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "else:\n",
    "    print(\"Local dataset not found or inaccessible. Attempting to download.\")\n",
    "    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Print dataset shapes\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert to categorical\n",
    "y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "y_test = np_utils.to_categorical(y_test, NB_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize pixel values to range 0-1\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ----------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model (Deeper Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ----------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the deep network\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First convolutional block\n",
    "model.add(Conv2D(32, (3, 3), padding='same', input_shape=(IMG_ROWS, IMG_COLS, IMG_CHANNELS)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second convolutional block\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,676,842\n",
      "Trainable params: 1,676,842\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIM, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 107s 3ms/step - loss: 1.7809 - accuracy: 0.3589 - val_loss: 1.4577 - val_accuracy: 0.4708\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 97s 2ms/step - loss: 1.3221 - accuracy: 0.5317 - val_loss: 1.2896 - val_accuracy: 0.5439\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 101s 3ms/step - loss: 1.1083 - accuracy: 0.6094 - val_loss: 1.1387 - val_accuracy: 0.6069\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 101s 3ms/step - loss: 0.9673 - accuracy: 0.6629 - val_loss: 0.9407 - val_accuracy: 0.6754\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 98s 2ms/step - loss: 0.8761 - accuracy: 0.6946 - val_loss: 0.9058 - val_accuracy: 0.6883\n",
      "Epoch 6/20\n",
      "40000/40000 [==============================] - 99s 2ms/step - loss: 0.7979 - accuracy: 0.7203 - val_loss: 0.7704 - val_accuracy: 0.7356\n",
      "Epoch 7/20\n",
      "40000/40000 [==============================] - 100s 3ms/step - loss: 0.7404 - accuracy: 0.7418 - val_loss: 0.7867 - val_accuracy: 0.7299\n",
      "Epoch 8/20\n",
      "40000/40000 [==============================] - 99s 2ms/step - loss: 0.6946 - accuracy: 0.7590 - val_loss: 0.7915 - val_accuracy: 0.7358\n",
      "Epoch 9/20\n",
      "40000/40000 [==============================] - 102s 3ms/step - loss: 0.6529 - accuracy: 0.7738 - val_loss: 0.7405 - val_accuracy: 0.7500\n",
      "Epoch 10/20\n",
      "40000/40000 [==============================] - 101s 3ms/step - loss: 0.6228 - accuracy: 0.7832 - val_loss: 0.6939 - val_accuracy: 0.7654\n",
      "Epoch 11/20\n",
      "40000/40000 [==============================] - 97s 2ms/step - loss: 0.5988 - accuracy: 0.7921 - val_loss: 0.8137 - val_accuracy: 0.7265\n",
      "Epoch 12/20\n",
      "40000/40000 [==============================] - 99s 2ms/step - loss: 0.5795 - accuracy: 0.7990 - val_loss: 0.7728 - val_accuracy: 0.7617\n",
      "Epoch 13/20\n",
      "40000/40000 [==============================] - 101s 3ms/step - loss: 0.5631 - accuracy: 0.8083 - val_loss: 0.7377 - val_accuracy: 0.7703\n",
      "Epoch 14/20\n",
      "40000/40000 [==============================] - 97s 2ms/step - loss: 0.5455 - accuracy: 0.8144 - val_loss: 0.8864 - val_accuracy: 0.7627\n",
      "Epoch 15/20\n",
      "40000/40000 [==============================] - 100s 3ms/step - loss: 0.5424 - accuracy: 0.8160 - val_loss: 0.7931 - val_accuracy: 0.7769\n",
      "Epoch 16/20\n",
      "40000/40000 [==============================] - 99s 2ms/step - loss: 0.5381 - accuracy: 0.8186 - val_loss: 0.6817 - val_accuracy: 0.7755\n",
      "Epoch 17/20\n",
      "40000/40000 [==============================] - 96s 2ms/step - loss: 0.5240 - accuracy: 0.8222 - val_loss: 0.6790 - val_accuracy: 0.7825\n",
      "Epoch 18/20\n",
      "40000/40000 [==============================] - 98s 2ms/step - loss: 0.5253 - accuracy: 0.8232 - val_loss: 0.8548 - val_accuracy: 0.7328\n",
      "Epoch 19/20\n",
      "40000/40000 [==============================] - 100s 3ms/step - loss: 0.5277 - accuracy: 0.8258 - val_loss: 0.6715 - val_accuracy: 0.7807\n",
      "Epoch 20/20\n",
      "40000/40000 [==============================] - 99s 2ms/step - loss: 0.5260 - accuracy: 0.8268 - val_loss: 0.7612 - val_accuracy: 0.7733\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=NB_EPOCH,\n",
    "                    validation_split=VALIDATION_SPLIT,\n",
    "                    verbose=VERBOSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 15s 1ms/step\n",
      "Test score: 0.7819904750347138\n",
      "Test accuracy: 0.7656000256538391\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "score = model.evaluate(X_test, y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Save model architecture and weights\n",
    "model_json = model.to_json()\n",
    "with open(\"cifar10_architecture.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"cifar10_weights.h5\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ----------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ----------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the data generator to the training data\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "390/390 [==============================] - 125s 321ms/step - loss: 1.4307 - accuracy: 0.5085 - val_loss: 1.0896 - val_accuracy: 0.6558\n",
      "Epoch 2/20\n",
      "390/390 [==============================] - 117s 301ms/step - loss: 1.3486 - accuracy: 0.5355 - val_loss: 0.9003 - val_accuracy: 0.7046\n",
      "Epoch 3/20\n",
      "390/390 [==============================] - 118s 302ms/step - loss: 1.3138 - accuracy: 0.5484 - val_loss: 0.9610 - val_accuracy: 0.6777\n",
      "Epoch 4/20\n",
      "390/390 [==============================] - 126s 323ms/step - loss: 1.2964 - accuracy: 0.5544 - val_loss: 1.0202 - val_accuracy: 0.6794\n",
      "Epoch 5/20\n",
      "390/390 [==============================] - 119s 304ms/step - loss: 1.2695 - accuracy: 0.5624 - val_loss: 1.1143 - val_accuracy: 0.6170\n",
      "Epoch 6/20\n",
      "390/390 [==============================] - 128s 327ms/step - loss: 1.2601 - accuracy: 0.5679 - val_loss: 1.0907 - val_accuracy: 0.6354\n",
      "Epoch 7/20\n",
      "390/390 [==============================] - 119s 305ms/step - loss: 1.2507 - accuracy: 0.5716 - val_loss: 1.0070 - val_accuracy: 0.6718\n",
      "Epoch 8/20\n",
      "390/390 [==============================] - 120s 307ms/step - loss: 1.2430 - accuracy: 0.5734 - val_loss: 1.1252 - val_accuracy: 0.6669\n",
      "Epoch 9/20\n",
      "390/390 [==============================] - 124s 318ms/step - loss: 1.2451 - accuracy: 0.5736 - val_loss: 1.0122 - val_accuracy: 0.6637\n",
      "Epoch 10/20\n",
      "390/390 [==============================] - 118s 302ms/step - loss: 1.2321 - accuracy: 0.5753 - val_loss: 1.0377 - val_accuracy: 0.6445\n",
      "Epoch 11/20\n",
      "390/390 [==============================] - 137s 352ms/step - loss: 1.2337 - accuracy: 0.5785 - val_loss: 0.9836 - val_accuracy: 0.6678\n",
      "Epoch 12/20\n",
      "390/390 [==============================] - 120s 308ms/step - loss: 1.2295 - accuracy: 0.5820 - val_loss: 1.0293 - val_accuracy: 0.6790\n",
      "Epoch 13/20\n",
      "390/390 [==============================] - 121s 310ms/step - loss: 1.2282 - accuracy: 0.5800 - val_loss: 0.9885 - val_accuracy: 0.6643\n",
      "Epoch 14/20\n",
      "390/390 [==============================] - 121s 310ms/step - loss: 1.2310 - accuracy: 0.5763 - val_loss: 0.9527 - val_accuracy: 0.6907\n",
      "Epoch 15/20\n",
      "390/390 [==============================] - 120s 307ms/step - loss: 1.2257 - accuracy: 0.5805 - val_loss: 1.0659 - val_accuracy: 0.6708\n",
      "Epoch 16/20\n",
      "390/390 [==============================] - 125s 322ms/step - loss: 1.2247 - accuracy: 0.5827 - val_loss: 0.9253 - val_accuracy: 0.6914\n",
      "Epoch 17/20\n",
      "390/390 [==============================] - 119s 305ms/step - loss: 1.2309 - accuracy: 0.5802 - val_loss: 0.8977 - val_accuracy: 0.6999\n",
      "Epoch 18/20\n",
      "390/390 [==============================] - 122s 312ms/step - loss: 1.2321 - accuracy: 0.5803 - val_loss: 1.0679 - val_accuracy: 0.6620\n",
      "Epoch 19/20\n",
      "390/390 [==============================] - 121s 311ms/step - loss: 1.2326 - accuracy: 0.5812 - val_loss: 1.0571 - val_accuracy: 0.6728\n",
      "Epoch 20/20\n",
      "390/390 [==============================] - 120s 307ms/step - loss: 1.2328 - accuracy: 0.5803 - val_loss: 0.9952 - val_accuracy: 0.6675\n"
     ]
    }
   ],
   "source": [
    "# Augment and train the model\n",
    "history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=BATCH_SIZE),\n",
    "                              steps_per_epoch=X_train.shape[0] // BATCH_SIZE,\n",
    "                              epochs=NB_EPOCH,\n",
    "                              validation_data=(X_test, y_test),\n",
    "                              verbose=VERBOSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 15s 1ms/step\n",
      "Test score: 0.9951906910896301\n",
      "Test accuracy: 0.6675000190734863\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model with augmented data\n",
    "score = model.evaluate(X_test, y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethical and Privacy Implications of the Algorithm\n",
    "\n",
    "The convolutional neural network (CNN) designed for CIFAR-10 image recognition demonstrates the power of machine learning to categorize image data efficiently. However, its potential applications raise significant ethical and privacy concerns when similar algorithms are trained on sensitive datasets, such as facial images or personal identifiers.\n",
    "\n",
    "## Surveillance and Privacy Risks\n",
    "Training this algorithm on facial recognition datasets could enable intrusive surveillance systems capable of identifying and tracking individuals without their consent. The erosion of anonymity in public spaces, as highlighted by Hill (2020), demonstrates the dangers posed by facial recognition technologies when used by law enforcement or private companies. Such misuse risks creating a surveillance society where personal freedom and privacy are diminished.\n",
    "\n",
    "## Bias and Discrimination\n",
    "Algorithms are only as unbiased as the datasets used to train them. If the training data lacks diversity, the algorithm may exhibit discriminatory behavior. Buolamwini and Gebru (2018) revealed that facial recognition systems often misidentify individuals from minority groups, leading to potential harm and injustice. This highlights the importance of ensuring fairness and accountability in dataset selection and model training.\n",
    "\n",
    "## Misuse of Technology\n",
    "The potential misuse of algorithms like this in authoritarian regimes or by malicious actors further underscores the need for ethical guardrails. Stahl and Wright (2018) emphasize the importance of responsible research and innovation (RRI) frameworks to mitigate the unintended consequences of AI development.\n",
    "\n",
    "## Data Security Concerns\n",
    "Access to large datasets often containing sensitive information introduces risks of data breaches and unauthorized use. Proper anonymization and stringent security measures are essential to safeguard individual privacy and maintain public trust (Granados, 2016).\n",
    "\n",
    "These considerations underline the need for ethical guidelines, robust regulatory oversight, and transparent practices in AI development. Without these measures, the widespread deployment of AI systems could exacerbate existing societal inequities and undermine democratic values.\n",
    "\n",
    "References:\n",
    "\n",
    "Buolamwini, J., & Gebru, T. (2018). Gender shades: Intersectional accuracy disparities in commercial gender classification. Proceedings of Machine Learning Research.\n",
    "\n",
    "Granados, N. (2016, June 30). How Facebook biases your news feed. Forbes. Retrieved from https://www.forbes.com/sites/nelsongranados/2016/06/30/how-facebook-biases-your-news-feed/\n",
    "\n",
    "Hill, K. (2020, January 21). The secretive company that might end privacy as we know it. International New York Times. Retrieved from https://www.nytimes.com\n",
    "\n",
    "Stahl, B. C., & Wright, D. (2018). Ethics and privacy in AI and big data: Implementing responsible research and innovation. IEEE Security & Privacy, 16(3), 26–33.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
