{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-2 Assignment: Identifying Hand-written Digits\n",
    "### Marc Anthony Aradillas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 4s 83us/step - loss: 1.4829 - accuracy: 0.6231 - val_loss: 0.7584 - val_accuracy: 0.8286\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 5s 94us/step - loss: 0.6049 - accuracy: 0.8464 - val_loss: 0.4550 - val_accuracy: 0.8852\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 4s 92us/step - loss: 0.4398 - accuracy: 0.8801 - val_loss: 0.3710 - val_accuracy: 0.9019\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 4s 78us/step - loss: 0.3767 - accuracy: 0.8952 - val_loss: 0.3322 - val_accuracy: 0.9082\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 4s 74us/step - loss: 0.3415 - accuracy: 0.9025 - val_loss: 0.3055 - val_accuracy: 0.9147\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 4s 73us/step - loss: 0.3175 - accuracy: 0.9086 - val_loss: 0.2880 - val_accuracy: 0.9182\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 3s 71us/step - loss: 0.2989 - accuracy: 0.9137 - val_loss: 0.2727 - val_accuracy: 0.9224\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 4s 78us/step - loss: 0.2839 - accuracy: 0.9180 - val_loss: 0.2608 - val_accuracy: 0.9266\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 4s 77us/step - loss: 0.2714 - accuracy: 0.9217 - val_loss: 0.2505 - val_accuracy: 0.9298\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 4s 77us/step - loss: 0.2602 - accuracy: 0.9252 - val_loss: 0.2430 - val_accuracy: 0.9308\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 4s 76us/step - loss: 0.2501 - accuracy: 0.9285 - val_loss: 0.2341 - val_accuracy: 0.9335\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 3s 72us/step - loss: 0.2409 - accuracy: 0.9301 - val_loss: 0.2271 - val_accuracy: 0.9352\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 3s 71us/step - loss: 0.2325 - accuracy: 0.9334 - val_loss: 0.2227 - val_accuracy: 0.9367\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 3s 71us/step - loss: 0.2253 - accuracy: 0.9353 - val_loss: 0.2147 - val_accuracy: 0.9396\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 3s 69us/step - loss: 0.2181 - accuracy: 0.9375 - val_loss: 0.2082 - val_accuracy: 0.9411\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 3s 68us/step - loss: 0.2116 - accuracy: 0.9394 - val_loss: 0.2030 - val_accuracy: 0.9431\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 3s 69us/step - loss: 0.2055 - accuracy: 0.9414 - val_loss: 0.1981 - val_accuracy: 0.9445\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 4s 74us/step - loss: 0.1996 - accuracy: 0.9430 - val_loss: 0.1932 - val_accuracy: 0.9458\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 3s 73us/step - loss: 0.1941 - accuracy: 0.9432 - val_loss: 0.1894 - val_accuracy: 0.9467\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 4s 77us/step - loss: 0.1890 - accuracy: 0.9456 - val_loss: 0.1849 - val_accuracy: 0.9498\n",
      "10000/10000 [==============================] - 1s 54us/step\n",
      "Test score: 0.18599770209044217\n",
      "Test accuracy: 0.9463000297546387\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "np.random.seed(1671)  # for reproducibility\n",
    "\n",
    "# Network and training\n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10  # number of outputs = number of digits\n",
    "OPTIMIZER = SGD()  # optimizer\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT = 0.2  # how much TRAIN is reserved for VALIDATION\n",
    "\n",
    "# Data: shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "RESHAPED = 784\n",
    "\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Normalize\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices\n",
    "y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=OPTIMIZER,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
    "                    verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "# Evaluate the model\n",
    "score = model.evaluate(X_test, y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy rate for the training, validation, and test data sets with two hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 94.56%\n",
      "Validation Accuracy: 94.98%\n",
      "10000/10000 [==============================] - 0s 46us/step\n",
      "Test Accuracy: 94.63%\n"
     ]
    }
   ],
   "source": [
    "# Extract training accuracy from the history object\n",
    "training_accuracy = history.history['accuracy'][-1]  # Last epoch training accuracy\n",
    "print(f\"Training Accuracy: {training_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Extract validation accuracy from the history object\n",
    "validation_accuracy = history.history['val_accuracy'][-1]  # Last epoch validation accuracy\n",
    "print(f\"Validation Accuracy: {validation_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "test_score = model.evaluate(X_test, y_test, verbose=VERBOSE)\n",
    "test_accuracy = test_score[1]\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Reduced Hidden Neurons (N_HIDDEN = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                650       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 55,050\n",
      "Trainable params: 55,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 3s 70us/step - loss: 1.6130 - accuracy: 0.5730 - val_loss: 0.8518 - val_accuracy: 0.8209\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.6516 - accuracy: 0.8400 - val_loss: 0.4876 - val_accuracy: 0.8746\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.4651 - accuracy: 0.8754 - val_loss: 0.3997 - val_accuracy: 0.8880\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 3s 62us/step - loss: 0.3996 - accuracy: 0.8898 - val_loss: 0.3545 - val_accuracy: 0.9010\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 3s 56us/step - loss: 0.3636 - accuracy: 0.8985 - val_loss: 0.3291 - val_accuracy: 0.9068\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 3s 64us/step - loss: 0.3397 - accuracy: 0.9044 - val_loss: 0.3106 - val_accuracy: 0.9118\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 3s 68us/step - loss: 0.3212 - accuracy: 0.9094 - val_loss: 0.2967 - val_accuracy: 0.9158\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 3s 61us/step - loss: 0.3065 - accuracy: 0.9133 - val_loss: 0.2844 - val_accuracy: 0.9191\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 0.2938 - accuracy: 0.9168 - val_loss: 0.2750 - val_accuracy: 0.9213\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 3s 63us/step - loss: 0.2832 - accuracy: 0.9195 - val_loss: 0.2662 - val_accuracy: 0.9232\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 3s 64us/step - loss: 0.2732 - accuracy: 0.9228 - val_loss: 0.2582 - val_accuracy: 0.9254\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 2s 50us/step - loss: 0.2645 - accuracy: 0.9246 - val_loss: 0.2498 - val_accuracy: 0.9280\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 2s 51us/step - loss: 0.2565 - accuracy: 0.9271 - val_loss: 0.2442 - val_accuracy: 0.9301\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 2s 51us/step - loss: 0.2491 - accuracy: 0.9289 - val_loss: 0.2380 - val_accuracy: 0.9325\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 2s 50us/step - loss: 0.2424 - accuracy: 0.9308 - val_loss: 0.2348 - val_accuracy: 0.9332\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.2362 - accuracy: 0.9327 - val_loss: 0.2277 - val_accuracy: 0.9342\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 3s 71us/step - loss: 0.2305 - accuracy: 0.9335 - val_loss: 0.2243 - val_accuracy: 0.9356\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 6s 119us/step - loss: 0.2251 - accuracy: 0.9360 - val_loss: 0.2190 - val_accuracy: 0.9369\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 6s 125us/step - loss: 0.2198 - accuracy: 0.9373 - val_loss: 0.2161 - val_accuracy: 0.9371\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 6s 125us/step - loss: 0.2149 - accuracy: 0.9394 - val_loss: 0.2121 - val_accuracy: 0.9396\n",
      "10000/10000 [==============================] - 0s 45us/step\n",
      "Test Accuracy (N_HIDDEN=64): 0.9391000270843506\n"
     ]
    }
   ],
   "source": [
    "N_HIDDEN = 64  # Reduced number of neurons in each hidden layer\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))  # First hidden layer\n",
    "model.add(Activation('relu'))  # Activation function for the first hidden layer\n",
    "model.add(Dense(N_HIDDEN))  # Second hidden layer\n",
    "model.add(Activation('relu'))  # Activation function for the second hidden layer\n",
    "model.add(Dense(NB_CLASSES))  # Output layer\n",
    "model.add(Activation('softmax'))  # Softmax activation for multi-class classification\n",
    "model.summary()  # Print the model summary\n",
    "\n",
    "# Compile the model with loss function and optimizer\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=OPTIMIZER,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train,  # Training data\n",
    "                    batch_size=BATCH_SIZE,  # Batch size for training\n",
    "                    epochs=NB_EPOCH,  # Number of epochs\n",
    "                    verbose=VERBOSE,  # Verbosity of training output\n",
    "                    validation_split=VALIDATION_SPLIT)  # Validation split\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "score = model.evaluate(X_test, y_test, verbose=VERBOSE)\n",
    "print(\"Test Accuracy (N_HIDDEN=64):\", score[1])  # Print the test accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduced Hidden Neurons\n",
    "By reducing the number of neurons in the hidden layers to 64, the model's capacity to learn complex patterns is limited. This leads to lower accuracy on the training, validation, and test datasets. The reduced model is not able to effectively capture the intricacies of the MNIST dataset, which consists of the handwritten digits with subtle variations. This result aligns with the concepts discussed in Deep Learning with Keras (Chapter 1, Pages 22–24), where the authors emphasize the importance of having sufficient neurons in hidden layers to balance learning capacity and generalization. With fewer neurons, the model underfits the data, as it cannot extract and learn all the necessary patterns. As a result, both training and test accuracies drop compared to the baseline experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Default Hidden Neurons (N_HIDDEN = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 9s 178us/step - loss: 1.4810 - accuracy: 0.6183 - val_loss: 0.7559 - val_accuracy: 0.8382\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 8s 162us/step - loss: 0.5984 - accuracy: 0.8541 - val_loss: 0.4502 - val_accuracy: 0.8849\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 8s 172us/step - loss: 0.4346 - accuracy: 0.8841 - val_loss: 0.3691 - val_accuracy: 0.8988\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 8s 159us/step - loss: 0.3749 - accuracy: 0.8948 - val_loss: 0.3308 - val_accuracy: 0.9079\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 8s 162us/step - loss: 0.3418 - accuracy: 0.9032 - val_loss: 0.3071 - val_accuracy: 0.9135\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 5s 96us/step - loss: 0.3196 - accuracy: 0.9091 - val_loss: 0.2894 - val_accuracy: 0.9193\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 4s 88us/step - loss: 0.3021 - accuracy: 0.9142 - val_loss: 0.2769 - val_accuracy: 0.9218\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 5s 94us/step - loss: 0.2882 - accuracy: 0.9183 - val_loss: 0.2650 - val_accuracy: 0.9253\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 4s 91us/step - loss: 0.2760 - accuracy: 0.9212 - val_loss: 0.2558 - val_accuracy: 0.9281\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 4s 87us/step - loss: 0.2652 - accuracy: 0.9246 - val_loss: 0.2464 - val_accuracy: 0.9301\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 4s 87us/step - loss: 0.2552 - accuracy: 0.9275 - val_loss: 0.2394 - val_accuracy: 0.9311\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 4s 90us/step - loss: 0.2460 - accuracy: 0.9301 - val_loss: 0.2325 - val_accuracy: 0.9328\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 5s 95us/step - loss: 0.2378 - accuracy: 0.9320 - val_loss: 0.2259 - val_accuracy: 0.9365\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 5s 95us/step - loss: 0.2298 - accuracy: 0.9346 - val_loss: 0.2190 - val_accuracy: 0.9389\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 5s 94us/step - loss: 0.2223 - accuracy: 0.9366 - val_loss: 0.2129 - val_accuracy: 0.9415\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 5s 95us/step - loss: 0.2155 - accuracy: 0.9388 - val_loss: 0.2064 - val_accuracy: 0.9422\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 5s 96us/step - loss: 0.2089 - accuracy: 0.9406 - val_loss: 0.2023 - val_accuracy: 0.9439\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 4s 75us/step - loss: 0.2029 - accuracy: 0.9428 - val_loss: 0.1986 - val_accuracy: 0.9448\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 3s 61us/step - loss: 0.1969 - accuracy: 0.9443 - val_loss: 0.1928 - val_accuracy: 0.9468\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 3s 67us/step - loss: 0.1916 - accuracy: 0.9456 - val_loss: 0.1875 - val_accuracy: 0.9489\n",
      "10000/10000 [==============================] - 0s 41us/step\n",
      "Test Accuracy (N_HIDDEN=128): 0.9447000026702881\n"
     ]
    }
   ],
   "source": [
    "N_HIDDEN = 128  # Default number of neurons in each hidden layer\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))  # First hidden layer\n",
    "model.add(Activation('relu'))  # Activation function for the first hidden layer\n",
    "model.add(Dense(N_HIDDEN))  # Second hidden layer\n",
    "model.add(Activation('relu'))  # Activation function for the second hidden layer\n",
    "model.add(Dense(NB_CLASSES))  # Output layer\n",
    "model.add(Activation('softmax'))  # Softmax activation for multi-class classification\n",
    "model.summary()  # Print the model summary\n",
    "\n",
    "# Compile the model with loss function and optimizer\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=OPTIMIZER,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train,  # Training data\n",
    "                    batch_size=BATCH_SIZE,  # Batch size for training\n",
    "                    epochs=NB_EPOCH,  # Number of epochs\n",
    "                    verbose=VERBOSE,  # Verbosity of training output\n",
    "                    validation_split=VALIDATION_SPLIT)  # Validation split\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "score = model.evaluate(X_test, y_test, verbose=VERBOSE)\n",
    "print(\"Test Accuracy (N_HIDDEN=128):\", score[1])  # Print the test accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Hidden Neurons\n",
    "\n",
    "The configuration with 128 hidden neurons represents the baseline setup discussed in the book (Deep Learning with Keras, Pages 22–23). The configuration achieves a balanced performance by providing the network with enough capacity to learn complex patterns without overfitting. The training and validation accuracies are closely aligned, indicating that the model generalizes well to unseen data. The test accuracy is high, demonstrating that the model is effective at recognizing handwritten digits. As noted in the book, the selection of this parameter strikes a balance between model complexity and the risk of overfitting, making it an ideal choice for datasets of moderate complexity like MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Increased Hidden Neurons (N_HIDDEN = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 10)                2570      \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 269,322\n",
      "Trainable params: 269,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 5s 110us/step - loss: 1.4291 - accuracy: 0.6431 - val_loss: 0.7164 - val_accuracy: 0.8475\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 5s 94us/step - loss: 0.5759 - accuracy: 0.8586 - val_loss: 0.4388 - val_accuracy: 0.8879\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 4s 93us/step - loss: 0.4238 - accuracy: 0.8874 - val_loss: 0.3615 - val_accuracy: 0.9023\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 4s 80us/step - loss: 0.3668 - accuracy: 0.8990 - val_loss: 0.3259 - val_accuracy: 0.9102\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 4s 86us/step - loss: 0.3348 - accuracy: 0.9058 - val_loss: 0.3035 - val_accuracy: 0.9150\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 4s 86us/step - loss: 0.3129 - accuracy: 0.9123 - val_loss: 0.2860 - val_accuracy: 0.9208\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 4s 82us/step - loss: 0.2955 - accuracy: 0.9159 - val_loss: 0.2719 - val_accuracy: 0.9246\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 4s 83us/step - loss: 0.2811 - accuracy: 0.9215 - val_loss: 0.2616 - val_accuracy: 0.9258\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 4s 86us/step - loss: 0.2686 - accuracy: 0.9247 - val_loss: 0.2517 - val_accuracy: 0.9296\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 4s 82us/step - loss: 0.2578 - accuracy: 0.9275 - val_loss: 0.2430 - val_accuracy: 0.9325\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 4s 85us/step - loss: 0.2480 - accuracy: 0.9302 - val_loss: 0.2344 - val_accuracy: 0.9339\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 4s 83us/step - loss: 0.2387 - accuracy: 0.9334 - val_loss: 0.2280 - val_accuracy: 0.9358\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 4s 83us/step - loss: 0.2304 - accuracy: 0.9354 - val_loss: 0.2197 - val_accuracy: 0.9390\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 4s 91us/step - loss: 0.2225 - accuracy: 0.9380 - val_loss: 0.2137 - val_accuracy: 0.9395\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 4s 93us/step - loss: 0.2151 - accuracy: 0.9397 - val_loss: 0.2071 - val_accuracy: 0.9417\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 5s 95us/step - loss: 0.2081 - accuracy: 0.9415 - val_loss: 0.2026 - val_accuracy: 0.9433\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 4s 81us/step - loss: 0.2019 - accuracy: 0.9435 - val_loss: 0.1977 - val_accuracy: 0.9443\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 4s 80us/step - loss: 0.1956 - accuracy: 0.9452 - val_loss: 0.1915 - val_accuracy: 0.9464\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 4s 82us/step - loss: 0.1900 - accuracy: 0.9466 - val_loss: 0.1870 - val_accuracy: 0.9477\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 4s 89us/step - loss: 0.1844 - accuracy: 0.9481 - val_loss: 0.1833 - val_accuracy: 0.9490\n",
      "10000/10000 [==============================] - 1s 55us/step\n",
      "Test Accuracy (N_HIDDEN=256): 0.9487000107765198\n"
     ]
    }
   ],
   "source": [
    "N_HIDDEN = 256  # Increased number of neurons in each hidden layer\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))  # First hidden layer\n",
    "model.add(Activation('relu'))  # Activation function for the first hidden layer\n",
    "model.add(Dense(N_HIDDEN))  # Second hidden layer\n",
    "model.add(Activation('relu'))  # Activation function for the second hidden layer\n",
    "model.add(Dense(NB_CLASSES))  # Output layer\n",
    "model.add(Activation('softmax'))  # Softmax activation for multi-class classification\n",
    "model.summary()  # Print the model summary\n",
    "\n",
    "# Compile the model with loss function and optimizer\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=OPTIMIZER,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train,  # Training data\n",
    "                    batch_size=BATCH_SIZE,  # Batch size for training\n",
    "                    epochs=NB_EPOCH,  # Number of epochs\n",
    "                    verbose=VERBOSE,  # Verbosity of training output\n",
    "                    validation_split=VALIDATION_SPLIT)  # Validation split\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "score = model.evaluate(X_test, y_test, verbose=VERBOSE)\n",
    "print(\"Test Accuracy (N_HIDDEN=256):\", score[1])  # Print the test accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increased Hidden Neurons\n",
    "\n",
    "Increasing the number of hidden neurons to 256 boosts the model's capacity to learn from the training data, leading to higher training accuracy. However, this increase also raises the risk of overfitting, as the model may begin to memorize the training data rather than generalizing to unseen examples. This is highlighted by a potential widening gap between training and validation accuracies. The book (Deep Learning with Keras, Page 24) discusses how larger models can improve performance but may require careful tuning to avoid overfitting. While the test accuracy might improve some, it can plateau or even decline if the model starts to overfit. This experiment demonstrates the trade-offs between increasing model complexity and maintaining generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "\n",
    "Across the three experiments, it is evident that the number of neurons in the hidden layers significantly impacts model performance. A smaller network with 64 neurons **underfits** the data, leading to **lower accuracy** across all datasets. The baseline setup with 128 neurons strikes a good balance, achieving **high accuracy** while avoiding overfitting. Increasing the number of neurons to 256 provides a **slight improvement** in training accuracy but risks overfitting, as discussed in Deep Learning with Keras. This analysis underscores the importance of tuning hyperparameters like the number of neurons to optimize performance based on the dataset's complexity and size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
